{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.structured_output import create_openai_fn_runnable, create_structured_output_runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 a. `create_openai_fn_runnable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'player_name': 'Ponting', 'action': 'hits a cover drive for a four', 'score': 4, 'critique': 'That went like a bullet!'}\n"
     ]
    }
   ],
   "source": [
    "class ExtractCommentaryDetails(BaseModel):\n",
    "    '''Extract details from cricket commentary.'''\n",
    "    player_name: str = Field(..., description=\"The name of the players\")\n",
    "    action: str = Field(..., description=\"The name of the shot played\")\n",
    "    score: Optional[int] = Field(None, description=\"Score achieved by the player\")\n",
    "    critique: Optional[str] = Field(None, description=\"The commentator's adjectives for the player\")\n",
    "\n",
    "llm = ChatOpenAI(api_key=API_KEY, model=\"gpt-4\", temperature=0)\n",
    "structured_llm = create_openai_fn_runnable([ExtractCommentaryDetails], llm)\n",
    "\n",
    "commentary = \"Ponting faces the ball, and hits a cover drive for a four, right above Rhodes' head! That went like a bullet!\"\n",
    "\n",
    "result = structured_llm.invoke(commentary)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 b. `create_structured_output_runnable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Statistical Inference', 'authors': 'George Casella and Roger Berger', 'publication_year': 2001, 'publisher': 'Cengage', 'other_details': 'The book starts with a quote from Sherlock Holmes.'}\n"
     ]
    }
   ],
   "source": [
    "class BookReader(BaseModel):\n",
    "    '''Identifying information about a research paper.'''\n",
    "    title: str = Field(..., description=\"The title of the book\")\n",
    "    authors: str = Field(..., description=\"The authors of the book\")\n",
    "    publication_year: int = Field(..., description=\"The year the book was published\")\n",
    "    publisher: str = Field(..., description=\"The publisher of the book\")\n",
    "    other_details: Optional[str] = Field(None, description=\"Other details about the book\")\n",
    "\n",
    "llm = ChatOpenAI(api_key=API_KEY, model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = create_structured_output_runnable(BookReader, llm, mode=\"openai-tools\", enforce_function_usage=True, return_single=True)\n",
    "\n",
    "input_text = \"\"\"\n",
    "\"Statistical Inference\" was written by George Casella and Roger Berger in 2001 by Cengage. The book starts with a quote from Sherlock Holmes.\"\"\"\n",
    "\n",
    "result = structured_llm.invoke(input_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 c. `create_stuff_documents_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"Mastering the Game of Go without Human Knowledge\" was written by David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis from DeepMind.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Load PDF document\n",
    "loader = PyMuPDFLoader(\"alphago_paper.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Who wrote the paper:\\n\\n{context}\")]\n",
    ")\n",
    "llm = ChatOpenAI(api_key=API_KEY, model=\"gpt-3.5-turbo\")\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "result = chain.invoke({\"context\": docs})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom chain using components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duelist\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.output_parsers.structured import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Memory store for creating documents on the fly\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"Jett is a duelist who can updraft and throw knives\", \"Omen is a controller who can create smokes and teleport\"],\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=API_KEY),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Template for creating structured output based on \"context\" and \"question\" variables\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please format the answer as a JSON object with a single key \"answer\".\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(openai_api_key=API_KEY)\n",
    "\n",
    "# Define the response schema\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"The answer to the question\", type=\"string\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Define the setup and retrieval function using RunnableParallel\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "# Create the full chain\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "# Invoking the chain with a Valorant-related question\n",
    "result = chain.invoke(\"What type of agent is Jett?\")\n",
    "\n",
    "print(result['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small-projects-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
