{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14637119",
   "metadata": {},
   "source": [
    "# ISML Tri1 Assignment 1 Workbook\n",
    "\n",
    "This is the code template for your Assignment 1. You need to add or modify code **only** in the space indicated for each question to get correct results. \n",
    "\n",
    "#### Do not modify any other code. Do not use any other libraries apart from those already in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c771af9",
   "metadata": {},
   "source": [
    "## Question 1a: Closed Form of Linear Regression derivation\n",
    "\n",
    "You tasks for Question 1 of this assignment are as follows:\n",
    "1. Read the specification below and complete the derivation of $\\frac{\\partial{J( {W})}}{\\partial{ {W}}}$, then compare the derivative to 0 to obtain W in matrix form, given X and Y. \n",
    "2. Answer the question in myUni Question 1a\n",
    "3. The implementation of weight calculation and cost is implemented in the code below, but there are some errors due to not following the specification of data given below. Correct the errors in the code within the indicated area.\n",
    "4. After error correction, copy the results to myUni assignment Question 1b space.\n",
    "5. Asnwer question 1c in myUni.\n",
    "\n",
    "Linear regression can be expressed as $y_k=w_0\\times 1 + w_1x_{k1}+...+w_{n-1}x_{k,n-1}$, <br>\n",
    "where the training dataset $( {X})$ has $m$ examples and $n$ features (where the first feature, for the intercept weight $w_0$, is 1).</br>\n",
    "The ground truth ${Y}$ has $m$ target values. </br>\n",
    "Equivalent matrix form is $\\hat{ {Y}}= {W^T} {X} $, where $X$ is a $n \\times m$ matrix, and $ {W}$ is a $n \\times 1$ vector of parameters, and $\\hat{ {Y}}$ is the vector of predicted values y.\n",
    "\n",
    "The cost function is $$J( {W}) = \\frac{1}{2m} \\sum\\limits_{i=0}^m(\\hat{y_i} - y_i)^2 \\tag{1}$$\n",
    "In the matrix form the cost function is $$J( {W})=\\frac{1}{2m}||\\hat{ {Y}} -  {Y}||_2^2$$\n",
    "\n",
    "In order to find $ {W}$, we need to calculate the derivative of $J$ with respect to $ {W}$: $$\\frac{\\partial{J( {W})}}{\\partial{ {W}}}=\\nabla_w(J( {W}))$$ and compare it to zero. \n",
    "\n",
    "**Your task is to prove that** $ {W}=( {X} {X}^T)^{-1} {X} {Y}^T$ </br>\n",
    "Note, that the data specification above and the W matrix formula is a little different than one discussed in lecture week 2. </br>\n",
    "This form is a little different than in the lecture, but equivalent for given data specification. </br>\n",
    "**Important: comment and justify every transformation of symbols and every step**\n",
    "\n",
    "\n",
    "For this derivation, you may need the following rules ($A$ is a matrix of constants, not containing $W$):<br> \n",
    "1. $ \\nabla_w(W^TAW)=2AW$ (if $A$ is symetrical), <br>\n",
    "2. $ A^T = A$, if $A$ evaluates to a scalar.\n",
    "3. $ \\nabla_w(AW)=A^T $\n",
    "\n",
    "When using Python and numpy, be aware of the differences between multiply, matmul, @, np.dot and * functions and operators when applied to matrices.\n",
    "\n",
    "Do not use any other imports apart from ones already in this question's code. <br>\n",
    "\n",
    "======= put your derivation here or latex or image in myUni Question 1a, starting from the following equation =================== <br>\n",
    "\n",
    "$$\\nabla_W(J( {W}))=\\frac{\\partial}{\\partial W}\\frac{1}{2m}||\\hat{ {Y}} -  {Y}||_2^2 = \\frac{\\partial}{\\partial W}\\frac{1}{2m}( \\hat{Y} -  {Y}) ( \\hat{Y} -  {Y})^T $$ \n",
    "\n",
    "\n",
    "\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2f611",
   "metadata": {},
   "source": [
    "## Question 1b: Closed Form of Linear Regression coding\n",
    "\n",
    "Correct the code in marked space to agree with the close form derivation specified above, run the code and enter the results in myUni Question 1b space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27925d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "J = 0.0\n",
    "\n",
    "\"\"\"\n",
    "Data for this question is as follows:\n",
    "\n",
    "x1     x2    y\n",
    "----------------\n",
    "3      5     13\n",
    "6      7     8\n",
    "7      8     11\n",
    "8      9     2\n",
    "11     11    6\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ====================== CORRECT THE CODE HERE ONLY ======================  \n",
    "# NOTE: to print correctly, sum(W) and J must be scalar float\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# incorrect implementation (students to correct)\n",
    "X = np.array([[3, 6, 7, 8, 11],[5, 7, 8, 9, 11]])\n",
    "W = np.array([[0], [0], [0]])\n",
    "Y = np.array([[13, 8, 11, 2, 6]])\n",
    "W = np.linalg.inv(X @ X.T) @ X @ Y.T\n",
    "J = 1/(2*Y.shape[1])*(X.T @ W - Y) @ (X.T @ W - Y).T\n",
    "#============================================================\n",
    "\n",
    "print('Please copy the folowing result to Question 1b in Assignment 1')\n",
    "print(np.round(np.sum(W) + J,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e00ff6",
   "metadata": {},
   "source": [
    "## Question 2: Linear Regression with Gradient Descent\n",
    "\n",
    "Implement linear regression with gradient descent using the below code template. Put your code in provided spaces to obtain correct results. <br>\n",
    "\n",
    "Use the formula of the Linear Regression cost and gradient descent **from Lecture week 2**.\n",
    "\n",
    "**Dataset**: The dataset used for this question is based on diabetes dataset posted here: https://data.mendeley.com/datasets/wj9rwkp9c2/1, However, this dataset is not identical to the original one. </br>\n",
    "The columns of the dataset used for this question are: age, bmi and bp (blood pressure).\n",
    "The dataset has m=442 instances, and n=3 attributes (the values of these attributes are features in each instance). </br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3795cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT modify code in this cell\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "dataset = pd.read_csv(\"diabetes1.csv\")\n",
    "\n",
    "# selecting features\n",
    "X_train = np.array(dataset[[\"age\", \"bmi\", \"bp\"]])\n",
    "\n",
    "# normalising numerical features\n",
    "X_train = (X_train-np.min(X_train,axis=0))/(np.max(X_train,axis=0)-np.min(X_train,axis=0))\n",
    "\n",
    "# adding '1' column for the intercept\n",
    "X_train = np.concatenate((np.ones(X_train.shape[0]).reshape(X_train.shape[0],1), X_train), axis=1)\n",
    "\n",
    "# forming target\n",
    "Y_train = np.array([dataset[\"target\"]])\n",
    "Y_train = Y_train.reshape(X_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2207a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_compute_cost(X, Y, W): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      Y (ndarray (m,1)) : target values\n",
    "      W (ndarray (n,1)) : model parameters  \n",
    "      \n",
    "    Returns:\n",
    "      J (scalar): cost\n",
    "    \"\"\"\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# ============================================================\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_compute_gradient(X, Y, W): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X : Data,m examples with n features\n",
    "      Y : m target values\n",
    "      W : n model parameters \n",
    "      \n",
    "    Returns:\n",
    "      dJ_dW : The gradient of the cost w.r.t. the parameters W, n values. \n",
    "    \"\"\"\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "    return dJ_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_gradient_descent(X, Y, W_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X                   : Data, n examples with m features\n",
    "      Y                   : m target values\n",
    "      W_in                : n initial model parameters  \n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      W                   : final n values of parameters \n",
    "      J (scalar)          : final cost\n",
    "      \"\"\"\n",
    "    \n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "    return W, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b86bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "initial_W = np.ones(X_train.shape[1]).reshape(X_train.shape[1],1)\n",
    "\n",
    "# gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "\"\"\"\n",
    "Apply functions coded above to calculate final W and cost J\n",
    "Use given datasets and parameters\n",
    "\"\"\"\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "# NOTE: to print correctly, sum(W) and J must be scalar float\n",
    "\n",
    "# run linreg_gradient_descent\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "W = W.T[0]\n",
    "J = J[0,0]\n",
    "\n",
    "print(\"Please copy the folowing result into Question 2 in myUni Assignment Question 2\")\n",
    "print(np.round(np.sum(W)+J,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fc6a0",
   "metadata": {},
   "source": [
    "## Question 3: Regularised Logistic Regression\n",
    "\n",
    "Now you will implement regularized logistic regression. Complete the code where indicated.\n",
    "\n",
    "Recall from Lecture week 4 that the regularized cost function in logistic regression for m instances and n features is\n",
    "\n",
    "$$ J(W) = \\frac{1}{m} \\left[ -Y \\log\\left(h\\left( X \\right) \\right) - \\left( 1 - Y\\right) \\log \\left( 1 - h\\left( X \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n w_j^2 $$\n",
    "\n",
    "\n",
    "predict $ y = 1$ if $‚Ñé_w (ùëß)‚â•0.5, ùëß= w^‚ä§ùë• \\ge 0 $ <br>\n",
    "predict $y = 0$ if $‚Ñé_w (ùëß)<0.5, ùëß= w^‚ä§ùë•<0 $\n",
    "\n",
    "m is number of instances. Note that you should not regularize the parameters $w_0$. \n",
    "\n",
    "Please use the formulas from Lecture week 4 to complete the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ed8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # convert input to a numpy array\n",
    "    z = np.array(z).astype(\"float\")\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================  \n",
    "    # DO NOT use any other import statements for this question\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8aaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT modify code in this cell\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "data = df[['Survived', 'Sex', 'Age', 'SibSp', 'Parch']].dropna()\n",
    "data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "data = np.array(data)\n",
    "X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "# normalise all cols\n",
    "for c in range(X.shape[1]):\n",
    "    X[:,c] = (X[:,c] - min(X[:,c]))/(max(X[:,c]) - min(X[:,c]))\n",
    "    \n",
    "# break into train/test\n",
    "split = int(0.8 * data.shape[0])\n",
    "\n",
    "X_train = X[:split]\n",
    "X_test = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_test = Y[split:]\n",
    "\n",
    "# Add intercept term to X\n",
    "X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n",
    "X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_costFunctionReg(W, X, Y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with L2 regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : Logistic regression vector of n parameters,\n",
    "        where n is the number of features including intercept.\n",
    "    \n",
    "    X : The data set with shape (m,n). m is the number of examples, and\n",
    "        n is the number of features.\n",
    "    \n",
    "    Y : The vector of data labels of size m.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the L2 regularized cost function. \n",
    "   \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = Y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea96064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_GradFunctionReg(W, X, Y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : Logistic regression vector of n parameters,\n",
    "        where n is the number of features including intercept.\n",
    "    \n",
    "    X : The data set with shape (m,n). m is the number of examples, and\n",
    "        n is the number of features.\n",
    "    \n",
    "    Y : The vector of data labels of size m.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : A vector of size m which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = Y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    grad = np.zeros(W.shape)\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "\n",
    "    # =============================================================\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39890595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_gradient_descent_reg(X, Y, W, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X                   : Data ndarray array, n examples with m features\n",
    "      Y                   : ndarray vector of target n values\n",
    "      W                   : ndarray vector of initial m model parameters \n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      W (ndarray)         : Updated values of parameters \n",
    "      \"\"\"\n",
    "    \n",
    "    # ===================== YOUR CODE HERE ======================\n",
    " \n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return J, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef227feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "initial_W = np.array([-40]*X_train.shape[1])\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 20000\n",
    "alpha = 0.02\n",
    "lambda_ = 2\n",
    "W = 0\n",
    "J = 0\n",
    "acc = 0\n",
    "\n",
    "\"\"\"\n",
    "Apply functions coded above to calculate:\n",
    "    final W after training,\n",
    "    cost J for training set after training\n",
    "    accuracy for test set\n",
    "Use given datasets and parameters\n",
    "\"\"\"\n",
    "\n",
    "# ===================== YOUR CODE HERE ======================\n",
    "# NOTE: to print correctly, sum(W), J and acc must be scalar floats\n",
    "\n",
    "# run gradient descent with regularisation\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "print('Please copy the folowing result into Question 3 in myUni Assignment Question 3')\n",
    "print(np.round(np.sum(W)+J+acc, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ac2ec",
   "metadata": {},
   "source": [
    "## Question 4: Implementing regression k-NN \n",
    "\n",
    "The question for this code consists of two parts:\n",
    "1. Full implementation of classification k-NN\n",
    "2. A template for your implementation of regression k-NN based on the classification k-NN\n",
    "\n",
    "Your tasks are to:\n",
    "1. Apply minimal changes to the indicated code to implement regression k-NN as presented in Lecture 3\n",
    "2. Train and test your regression code on the dataset provided.\n",
    "3. Enter the RMSE result in myUni Assignment 1, Question 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT modify code in this cell\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other  statements for this question\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "data = df[['Survived', 'Sex', 'Age', 'SibSp', 'Parch']].dropna()\n",
    "data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "data = np.array(data)\n",
    "X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "# normalise all cols\n",
    "for c in range(X.shape[1]):\n",
    "    X[:,c] = (X[:,c] - min(X[:,c]))/(max(X[:,c]) - min(X[:,c]))\n",
    "    \n",
    "# break into train/test\n",
    "split = int(0.8 * data.shape[0])\n",
    "\n",
    "X_train = X[:split]\n",
    "X_test = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_test = Y[split:]\n",
    "\n",
    "# Add intercept term to X\n",
    "X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n",
    "X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376cf2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT modify code in this cell\n",
    "\n",
    "\"\"\"\n",
    "def distance(x1, x2):\n",
    "    \"\"\"\n",
    "    Calculates Euclidean distance between two point x1 and x2.\n",
    "    \n",
    "    Args:\n",
    "      x1                  : Data point, a numeric vector of size n (number of features)\n",
    "      x2                  : Data point, a numeric vector of size n (number of features)\n",
    "      \n",
    "    Returns:\n",
    "      d (float)           : Euclidean distance (scalar) between x1 and x2 in feature space \n",
    "      \"\"\"\n",
    "    \n",
    "    d = np.linalg.norm(x1-x2)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT modify code in this cell\n",
    "\n",
    "\"\"\"\n",
    "def knn(X_train, Y_train, X_test, k=9):\n",
    "    \"\"\"\n",
    "    Trains the k nearest neighbour model using X_train, Y_train dataset, \n",
    "    then tests the model on X_test, and returns predictions for the test set.\n",
    "    \n",
    "    Args:\n",
    "      X_train  : Training data, ndarray array, n examples with m features\n",
    "      Y_train  : ndarray vector of target n values\n",
    "      X_test   : Test data, ndarray array, n_test examples with m features\n",
    "      k (int)  : number of neighbour points to be used by k-NN\n",
    "      \n",
    "    Returns:\n",
    "      Y_pred   : ndarray vector of predicted n values, one predicted value for each test example\n",
    "      \"\"\"\n",
    "\n",
    "    Y_pred = []\n",
    "    for ix in range(X_test.shape[0]): # all train examples\n",
    "        vals = []\n",
    "        for jx in range(X_train.shape[0]): # all test examples\n",
    "            d = distance(X_test[ix], X_train[jx]) # distances of all training points from xt\n",
    "            vals.append([d, Y_train[jx]]) #dist, target class values\n",
    "        \n",
    "        # sorted labels has two cols: dist and class of each point that dist was calculated\n",
    "        sorted_labels = sorted(vals, key=lambda z: z[0]) # sort by dist, first val, second would be z[1], which is y value\n",
    "        neighbours = np.asarray(sorted_labels)[:k, -1] # take k rows of last col (y val)\n",
    "\n",
    "        # count of each val Y, there are two vals 0 amd 1\n",
    "        freq = np.unique(neighbours, return_counts=True) \n",
    "\n",
    "        y_pred = freq[0][freq[1].argmax()]\n",
    "        Y_pred.append(y_pred)\n",
    "    Y_pred = np.array(Y_pred)\n",
    "        \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT modify code in this cell\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "k=9\n",
    "\n",
    "Y_pred = knn(X_train, Y_train, X_test, k)\n",
    "rec = np.sum(Y_test + Y_pred == 2)/np.sum(Y_test)\n",
    "prec = np.sum(Y_test + Y_pred == 2)/Y_pred.shape[0]\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "accuracy = np.mean(Y_test == Y_pred)\n",
    "\n",
    "\n",
    "print(np.round(accuracy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487bcac",
   "metadata": {},
   "source": [
    "Apply minimal number of changes to the above code to make it work as regression k-NN, rather than classification. Use discussion in Lecture week 3 on the concept of regression k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_reg(X_train, Y_train, X_test, k=9):\n",
    "    \"\"\"\n",
    "    Trains the k nearest neighbour model using X_train, Y_train dataset, \n",
    "    then tests the model on X_test, and returns predictions for the test set.\n",
    "    \n",
    "    Args:\n",
    "      X_train  : Training data, ndarray array, n examples with m features\n",
    "      Y_train  : ndarray vector of target n values\n",
    "      X_test   : Test data, ndarray array, n_test examples with m features\n",
    "      k (int)  : number of neighbour points to be used by k-NN\n",
    "      \n",
    "    Returns:\n",
    "      Y_pred   : ndarray vector of predicted n values, one predicted value for each test example\n",
    "      \"\"\"\n",
    "\n",
    "    Y_pred = []\n",
    "    \n",
    "    # ===================== This is a copy of the knn function above ======================\n",
    "    # ===================== Modify this code to make it regression k-NN ======================\n",
    "    for ix in range(X_test.shape[0]): # all train examples\n",
    "        vals = []\n",
    "        for jx in range(X_train.shape[0]): # all test examples\n",
    "            d = distance(X_test[ix], X_train[jx]) # distances of all training points from xt\n",
    "            vals.append([d, Y_train[jx]]) #dist, target class values\n",
    "        \n",
    "        # sorted labels has two cols: dist and class of each point that dist was calculated\n",
    "        sorted_labels = sorted(vals, key=lambda z: z[0]) # sort by dist, first val, second would be z[1], which is y value\n",
    "        neighbours = np.asarray(sorted_labels)[:k, -1] # take k rows of last col (y val)\n",
    "\n",
    "        # count of each val Y, there are two vals 0 amd 1\n",
    "        freq = np.unique(neighbours, return_counts=True) \n",
    "\n",
    "        y_pred = freq[0][freq[1].argmax()]\n",
    "        Y_pred.append(y_pred)\n",
    "    Y_pred = np.array(Y_pred)\n",
    "\n",
    "    # ============================================================= \n",
    "        \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "dataset = pd.read_csv(\"diabetes1.csv\")\n",
    "\n",
    "# selecting features\n",
    "X = np.array(dataset[[\"age\", \"bmi\", \"bp\"]])\n",
    "\n",
    "# forming target\n",
    "Y = np.array([dataset[\"target\"]])\n",
    "\n",
    "Y = Y.reshape(Y.shape[1],)\n",
    "\n",
    "# break into train/test\n",
    "split = int(0.8 * Y.shape[0])\n",
    "\n",
    "X_train = X[:split]\n",
    "X_test = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_test = Y[split:]\n",
    "\n",
    "k = 5\n",
    "\n",
    "# ===================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "print('Please copy the folowing result into Question 4 in myUni Assignment Question 4')\n",
    "print(np.round(rmse,2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
